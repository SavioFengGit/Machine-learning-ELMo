{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Elmo-K-7-10.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"I4IF9P8ChfN_","executionInfo":{"status":"ok","timestamp":1623107200563,"user_tz":-120,"elapsed":346,"user":{"displayName":"Felipe Matè","photoUrl":"","userId":"07797625908934989507"}},"outputId":"2de7585a-f0a0-47ca-9f71-4e7ccf90ba51"},"source":["!nvidia-smi"],"execution_count":null,"outputs":[{"output_type":"stream","text":["NVIDIA-SMI has failed because it couldn't communicate with the NVIDIA driver. Make sure that the latest NVIDIA driver is installed and running.\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"dEVLBzZ3pzOw","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1623495411594,"user_tz":-120,"elapsed":41388,"user":{"displayName":"Xiao Li Savio Feng","photoUrl":"","userId":"06614088774789173560"}},"outputId":"40300a73-4dca-4866-c74c-9cecad88e1e8"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"3QoIEjtk0q3B"},"source":[" # Algoritmo"]},{"cell_type":"markdown","metadata":{"id":"C-AmvDIIzS2w"},"source":["## Install"]},{"cell_type":"code","metadata":{"id":"pm0YbhvKzTBF","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1623495421778,"user_tz":-120,"elapsed":3470,"user":{"displayName":"Xiao Li Savio Feng","photoUrl":"","userId":"06614088774789173560"}},"outputId":"c5599129-706e-4ed4-fa78-cbae68831123"},"source":["pip install 'h5py<3.0.0'"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting h5py<3.0.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3f/c0/abde58b837e066bca19a3f7332d9d0493521d7dd6b48248451a9e3fe2214/h5py-2.10.0-cp37-cp37m-manylinux1_x86_64.whl (2.9MB)\n","\u001b[K     |████████████████████████████████| 2.9MB 27.3MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.7/dist-packages (from h5py<3.0.0) (1.19.5)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from h5py<3.0.0) (1.15.0)\n","\u001b[31mERROR: tensorflow 2.5.0 has requirement h5py~=3.1.0, but you'll have h5py 2.10.0 which is incompatible.\u001b[0m\n","Installing collected packages: h5py\n","  Found existing installation: h5py 3.1.0\n","    Uninstalling h5py-3.1.0:\n","      Successfully uninstalled h5py-3.1.0\n","Successfully installed h5py-2.10.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"d8mhNt1sesQM","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1623495526867,"user_tz":-120,"elapsed":105110,"user":{"displayName":"Xiao Li Savio Feng","photoUrl":"","userId":"06614088774789173560"}},"outputId":"d2b219a7-1bc3-482f-fd8c-410206bbc283"},"source":["pip install tensorflow==1.15.0"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting tensorflow==1.15.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/92/2b/e3af15221da9ff323521565fa3324b0d7c7c5b1d7a8ca66984c8d59cb0ce/tensorflow-1.15.0-cp37-cp37m-manylinux2010_x86_64.whl (412.3MB)\n","\u001b[K     |████████████████████████████████| 412.3MB 44kB/s \n","\u001b[?25hRequirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (1.34.1)\n","Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (0.12.0)\n","Collecting keras-applications>=1.0.8\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/e3/19762fdfc62877ae9102edf6342d71b28fbfd9dea3d2f96a882ce099b03f/Keras_Applications-1.0.8-py3-none-any.whl (50kB)\n","\u001b[K     |████████████████████████████████| 51kB 8.3MB/s \n","\u001b[?25hRequirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (1.19.5)\n","Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (0.8.1)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (3.3.0)\n","Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (0.2.0)\n","Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (0.36.2)\n","Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (1.1.2)\n","Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (3.12.4)\n","Collecting tensorflow-estimator==1.15.1\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/de/62/2ee9cd74c9fa2fa450877847ba560b260f5d0fb70ee0595203082dafcc9d/tensorflow_estimator-1.15.1-py2.py3-none-any.whl (503kB)\n","\u001b[K     |████████████████████████████████| 512kB 54.3MB/s \n","\u001b[?25hRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (1.1.0)\n","Collecting gast==0.2.2\n","  Downloading https://files.pythonhosted.org/packages/4e/35/11749bf99b2d4e3cceb4d55ca22590b0d7c2c62b9de38ac4a4a7f4687421/gast-0.2.2.tar.gz\n","Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (1.12.1)\n","Collecting tensorboard<1.16.0,>=1.15.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1e/e9/d3d747a97f7188f48aa5eda486907f3b345cd409f0a0850468ba867db246/tensorboard-1.15.0-py3-none-any.whl (3.8MB)\n","\u001b[K     |████████████████████████████████| 3.8MB 48.3MB/s \n","\u001b[?25hRequirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (1.15.0)\n","Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras-applications>=1.0.8->tensorflow==1.15.0) (2.10.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.6.1->tensorflow==1.15.0) (57.0.0)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.0) (3.3.4)\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.0) (1.0.1)\n","Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.0) (4.0.1)\n","Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.0) (3.7.4.3)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.0) (3.4.1)\n","Building wheels for collected packages: gast\n","  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for gast: filename=gast-0.2.2-cp37-none-any.whl size=7557 sha256=a58811579a9408d7d43eae93d62997a54d16d3076e85b69d7e4c3985ea531bbc\n","  Stored in directory: /root/.cache/pip/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd\n","Successfully built gast\n","\u001b[31mERROR: tensorflow-probability 0.12.1 has requirement gast>=0.3.2, but you'll have gast 0.2.2 which is incompatible.\u001b[0m\n","\u001b[31mERROR: kapre 0.3.5 has requirement tensorflow>=2.0.0, but you'll have tensorflow 1.15.0 which is incompatible.\u001b[0m\n","Installing collected packages: keras-applications, tensorflow-estimator, gast, tensorboard, tensorflow\n","  Found existing installation: tensorflow-estimator 2.5.0\n","    Uninstalling tensorflow-estimator-2.5.0:\n","      Successfully uninstalled tensorflow-estimator-2.5.0\n","  Found existing installation: gast 0.4.0\n","    Uninstalling gast-0.4.0:\n","      Successfully uninstalled gast-0.4.0\n","  Found existing installation: tensorboard 2.5.0\n","    Uninstalling tensorboard-2.5.0:\n","      Successfully uninstalled tensorboard-2.5.0\n","  Found existing installation: tensorflow 2.5.0\n","    Uninstalling tensorflow-2.5.0:\n","      Successfully uninstalled tensorflow-2.5.0\n","Successfully installed gast-0.2.2 keras-applications-1.0.8 tensorboard-1.15.0 tensorflow-1.15.0 tensorflow-estimator-1.15.1\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"U6QqdON1et24","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1623495533159,"user_tz":-120,"elapsed":6325,"user":{"displayName":"Xiao Li Savio Feng","photoUrl":"","userId":"06614088774789173560"}},"outputId":"3354d9a0-aeca-4508-e481-5be09f118bf2"},"source":["pip install keras==2.5.0rc0"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting keras==2.5.0rc0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5c/1f/5ebba489d726f089d17591d56af3756a1261371f795da0e7f23dc9b4d8ac/keras-2.5.0rc0-py2.py3-none-any.whl (1.2MB)\n","\u001b[K     |████████████████████████████████| 1.2MB 26.5MB/s \n","\u001b[?25hInstalling collected packages: keras\n","  Found existing installation: Keras 2.4.3\n","    Uninstalling Keras-2.4.3:\n","      Successfully uninstalled Keras-2.4.3\n","Successfully installed keras-2.5.0rc0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"TpH69jruevEv","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1623495536424,"user_tz":-120,"elapsed":3315,"user":{"displayName":"Xiao Li Savio Feng","photoUrl":"","userId":"06614088774789173560"}},"outputId":"83d1d139-5cdb-4f0f-c8d6-8e396b886941"},"source":["!pip install wordninja"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting wordninja\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/30/15/abe4af50f4be92b60c25e43c1c64d08453b51e46c32981d80b3aebec0260/wordninja-2.0.0.tar.gz (541kB)\n","\r\u001b[K     |▋                               | 10kB 22.9MB/s eta 0:00:01\r\u001b[K     |█▏                              | 20kB 27.8MB/s eta 0:00:01\r\u001b[K     |█▉                              | 30kB 18.1MB/s eta 0:00:01\r\u001b[K     |██▍                             | 40kB 15.5MB/s eta 0:00:01\r\u001b[K     |███                             | 51kB 17.6MB/s eta 0:00:01\r\u001b[K     |███▋                            | 61kB 17.3MB/s eta 0:00:01\r\u001b[K     |████▎                           | 71kB 18.6MB/s eta 0:00:01\r\u001b[K     |████▉                           | 81kB 18.7MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 92kB 19.8MB/s eta 0:00:01\r\u001b[K     |██████                          | 102kB 20.9MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 112kB 20.9MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 122kB 20.9MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 133kB 20.9MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 143kB 20.9MB/s eta 0:00:01\r\u001b[K     |█████████                       | 153kB 20.9MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 163kB 20.9MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 174kB 20.9MB/s eta 0:00:01\r\u001b[K     |███████████                     | 184kB 20.9MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 194kB 20.9MB/s eta 0:00:01\r\u001b[K     |████████████                    | 204kB 20.9MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 215kB 20.9MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 225kB 20.9MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 235kB 20.9MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 245kB 20.9MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 256kB 20.9MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 266kB 20.9MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 276kB 20.9MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 286kB 20.9MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 296kB 20.9MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 307kB 20.9MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 317kB 20.9MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 327kB 20.9MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 337kB 20.9MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 348kB 20.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 358kB 20.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 368kB 20.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 378kB 20.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 389kB 20.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 399kB 20.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 409kB 20.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 419kB 20.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 430kB 20.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 440kB 20.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▋     | 450kB 20.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 460kB 20.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 471kB 20.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 481kB 20.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 491kB 20.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 501kB 20.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 512kB 20.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 522kB 20.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 532kB 20.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 542kB 20.9MB/s \n","\u001b[?25hBuilding wheels for collected packages: wordninja\n","  Building wheel for wordninja (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for wordninja: filename=wordninja-2.0.0-cp37-none-any.whl size=541553 sha256=440636c940cecf098cf1e9b01ff7e36e70172a3836b253e3608b4d3ef6247caf\n","  Stored in directory: /root/.cache/pip/wheels/22/46/06/9b6d10ed02c85e93c3bb33ac50e2d368b2586248f192a2e22a\n","Successfully built wordninja\n","Installing collected packages: wordninja\n","Successfully installed wordninja-2.0.0\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"a6qVtamazQWp"},"source":["## Imports"]},{"cell_type":"code","metadata":{"id":"qCHCGAvYzQcn"},"source":["import csv\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import pandas as pd\n","import seaborn as sn\n","import tensorflow_hub as hub\n","import tensorflow as tf\n","import tensorflow.keras\n","import wordninja as wn\n","\n","from sklearn import metrics\n","from sklearn import preprocessing\n","from sklearn.utils import shuffle\n","from sklearn.model_selection import train_test_split\n","from sklearn.model_selection import StratifiedKFold\n","\n","from tensorflow.keras.models import Model, load_model\n","from tensorflow.keras.layers import Dense,Lambda,Input\n","from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rhT9kfmOo00G"},"source":["path = \"/content/drive/MyDrive/Cyber Security/Elmo/prova_felipe/\"\n","elmo_path = \"/content/drive/MyDrive/Cyber Security/Elmo/prova_felipe/4/\"\n","\n","batch_size = 32\n","numEpochs = 10\n","\n","start_fold = 1\n","end_fold = 11\n","\n","nfolds = 10\n","\n","nome_file = \"Dataset_Completo.csv\""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1Md_C7LcsEL9"},"source":["# Preprocess"]},{"cell_type":"code","metadata":{"id":"SieB4yCVzD7O"},"source":["batch_size = 32\n","numEpochs = 10\n","\n","start_fold = 1\n","end_fold = 11\n","\n","nfolds = 10\n","\n","nome_file = \"Dataset_Completo.csv\"\n","\n","def arrayToSentence(x):\n","  string=''\n","  for a in x:\n","    string=string + a + ' '\n","  return string\n","\n","def buildDataset():\n","  filecsv = open(path + nome_file, newline=\"\")\n","  lettore = csv.reader(filecsv, delimiter=\";\")\n","\n","  dataset_x = []\n","  dataset_y = []\n","  temp_y = []\n","  for a in lettore:\n","    dataset_y.append(a[0])\n","    if a[0] == 'dga':\n","      temp_y.append(1)\n","    else:\n","      temp_y.append(0)\n","    split = wn.split(a[3])\n","    sen = arrayToSentence(split)\n","    dataset_x.append(sen)\n","\n","  filecsv.close()\n","\n","  return dataset_x, dataset_y, temp_y"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Wa3XafjoSk4p"},"source":["def kfold(x, y, temp_y):\n","  # Divide the dataset into training + holdout and testing with folds\n","  sss = StratifiedKFold(n_splits=nfolds)\n","\n","  fold = 0\n","  for train, test in sss.split(x, temp_y):\n","    print(\"Writing fold \" + str(fold + 1) + \" to csv...\")\n","    fold += 1\n","    x_train, x_test, y_train, y_test, y_temp_train, y_temp_test = x[train], x[test], y[train], y[test], temp_y[train], temp_y[test]\n","    np.savetxt(path + \"Dataset/x_train\" + str(fold) + \".csv\", x_train, fmt='%s', delimiter=';')\n","    np.savetxt(path + \"Dataset/x_test\" + str(fold) + \".csv\", x_test, fmt='%s', delimiter=';')\n","    np.savetxt(path + \"Dataset/y_train\" + str(fold) + \".csv\", y_train, fmt='%s', delimiter=';')\n","    np.savetxt(path + \"Dataset/y_test\" + str(fold) + \".csv\", y_test, fmt='%s', delimiter=';')\n","    np.savetxt(path + \"Dataset/temp_y_train\" + str(fold) + \".csv\", y_temp_train, fmt='%i', delimiter=';')\n","    np.savetxt(path + \"Dataset/temp_y_test\" + str(fold) + \".csv\", y_temp_test, fmt='%i', delimiter=';')\n","  print(\"Files created\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZeMs0jxwNXjX"},"source":["def encode(le, labels):\n","    enc = le.transform(labels)\n","    return tf.keras.utils.to_categorical(enc)  #era solo keras.utils...\n","\n","def decode(le, one_hot):\n","    dec = np.argmax(one_hot, axis=1)\n","    return le.inverse_transform(dec)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eLgkHmtRp-Hj"},"source":["dataset_x, dataset_y, temp_y = buildDataset()\n","dataset_x = np.array(dataset_x)\n","\n","le = preprocessing.LabelEncoder()\n","le.fit(dataset_y)\n","\n","dataset_y_encode = encode(le, dataset_y)\n","dataset_y = np.array(dataset_y_encode)\n","\n","temp_y = np.array(temp_y)\n","\n","kfold(dataset_x, dataset_y, temp_y)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pIxxcVXusLMn"},"source":["# Model"]},{"cell_type":"code","metadata":{"id":"XVZZfpNC0qlM","colab":{"base_uri":"https://localhost:8080/","height":710},"executionInfo":{"status":"error","timestamp":1623756197709,"user_tz":-120,"elapsed":227788,"user":{"displayName":"Dianel Ago","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjf5gQZ09EVXm1OIM0z43EfasF1DULFsEkx9h2qmg=s64","userId":"03980449275962121741"}},"outputId":"c8dd28a0-baf2-4a60-c859-ee04e495b573"},"source":["fold = 0\n","# sss = StratifiedKFold(n_splits=nfolds)\n","# for train, test in sss.split(dataset_x, temp_y):\n","#   fold += 1\n","#   print('Fold: ', fold)\n","#   x_train, x_test, y_train, y_test, y_train_temp, y_test_temp = dataset_x[train], dataset_x[test], dataset_y[train], dataset_y[test], temp_y[train], temp_y[test]\n","\n","for fold in range(start_fold, end_fold):\n","  print('Fold: ', fold, 'Epochs: ', numEpochs)\n","  #Get fold by csv\n","  x_train = np.genfromtxt(path + \"Dataset/x_train\" + str(fold) + \".csv\", delimiter=';', dtype=None)\n","  x_test = np.genfromtxt(path + \"Dataset/x_test\" + str(fold) + \".csv\", delimiter=';', dtype=None)\n","  y_train = np.genfromtxt(path + \"Dataset/y_train\" + str(fold) + \".csv\", delimiter=';', dtype=None)\n","  y_test = np.genfromtxt(path + \"Dataset/y_test\" + str(fold) + \".csv\", delimiter=';', dtype=None)\n","  y_train_temp = np.genfromtxt(path + \"Dataset/temp_y_train\" + str(fold) + \".csv\", delimiter=';', dtype=None)\n","  y_test_temp = np.genfromtxt(path + \"Dataset/temp_y_test\" + str(fold) + \".csv\", delimiter=';', dtype=None)\n","\n","  print('Model Construction...')\n","  model = None\n","\n","  #Parte costruzione del modello\n","  # importo il modulo con la funzione di embedding ELMo\n","  elmo = hub.Module(elmo_path)\n","\n","  # Definisco la funzione di embedding\n","  def ELMoEmbedding(x):\n","    elmoembed = elmo(tf.reshape(tf.cast(x, tf.string), [-1]), signature=\"default\", as_dict=True)[\"elmo\"]\n","    return elmoembed\n","\n","  input_text = Input(shape=(None,), dtype=tf.string)\n","  embedding = Lambda(ELMoEmbedding, output_shape=(1024, ))(input_text)\n","  dense = Dense(128, activation='relu')(embedding)\n","  pred = Dense(len(y_train[0]), activation='sigmoid')(dense)\n","  model = Model(inputs=[input_text], outputs=pred)\n","\n","  model.compile('adam', 'binary_crossentropy', metrics=['accuracy',\n","      tf.keras.metrics.AUC(),\n","      tf.keras.metrics.Precision(),\n","      tf.keras.metrics.Recall(),\n","      ])\n","\n","  print('...done!')\n","\n","  print('Train...')\n","  #parte di training\n","  \n","  Type = 'binary-'\n","  with tf.compat.v1.Session() as session:\n","    earlystop = EarlyStopping(monitor='loss', patience=3)\n","    best_save = ModelCheckpoint(path + 'Saved/bestmodel' + str(fold) + '.hdf5', save_best_only=True, \n","                                save_weights_only=False, \n","                                monitor='val_loss', \n","                                mode='min')\n","    tf.compat.v1.keras.backend.set_session(session)\n","    session.run(tf.compat.v1.global_variables_initializer())\n","    session.run(tf.compat.v1.tables_initializer())\n","    history = model.fit(x_train,y_train,\n","              batch_size=batch_size,\n","              epochs=numEpochs, \n","              callbacks=[earlystop, best_save],\n","              validation_split=0.1\n","              )\n","    # model.save_weights('/content/drive/MyDrive/Cyber Security/Elmo/prova_felipe/Saved_weights/' + Type + 'elmo-model' + str(fold) + '.h5')\n","\n","    print('\\nhistory dict:', history.history)\n","    \n","\n","  print('...done!')\n","\n","  print('Test...')\n","  #Parte di test\n","  with tf.compat.v1.Session() as session:\n","    tf.compat.v1.keras.backend.set_session(session)\n","    session.run(tf.compat.v1.global_variables_initializer())\n","    session.run(tf.compat.v1.tables_initializer())\n","    best_model = load_model(path + 'Saved/bestmodel' + str(fold) + '.hdf5')\n","    predicts = best_model.predict(x_test, batch_size=batch_size)\n","\n","  # print(predicts)\n","  y_preds = decode(le, predicts)\n","  y_test_temp = np.where(y_test_temp == 1, 'dga', 'legit')\n","  print('...done!')\n","\n","  print('Results:')\n","  #Plotta i risultati\n","  cm = metrics.confusion_matrix(y_test_temp, y_preds)\n","  np.savetxt(path + 'Saved/confusion_matrix' + str(fold) + '.csv', cm, delimiter=',',  fmt='%i')\n","  metrics1 = metrics.classification_report(y_test_temp, y_preds, output_dict=True, target_names=['legit', 'dga'])\n","  print('Confusion_matrix:')\n","  print(cm)\n","  print('Classification_report:')\n","  print(metrics1)\n","  try:\n","      df1 = pd.read_csv(path + \"Saved/metrics1.csv\", index_col=[0])\n","      df1 = df1.append(pd.DataFrame(metrics1))\n","      df1.to_csv(path + \"Saved/metrics1.csv\")\n","  except:\n","      pd.DataFrame(metrics1).to_csv(path + \"Saved/metrics1.csv\")\n","\n","  df_cm = pd.DataFrame(cm, index = [i for i in le.classes_],\n","                    columns = [i for i in le.classes_])\n","  plt.figure(1, figsize = (10,7))\n","  sn.heatmap(df_cm, annot=True, fmt=\"d\")\n","  plt.show()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Fold:  10 Epochs:  10\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: VisibleDeprecationWarning: Reading unicode strings without specifying the encoding argument is deprecated. Set the encoding, use None for the system default.\n","  # This is added back by InteractiveShellApp.init_path()\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:12: VisibleDeprecationWarning: Reading unicode strings without specifying the encoding argument is deprecated. Set the encoding, use None for the system default.\n","  if sys.path[0] == '':\n"],"name":"stderr"},{"output_type":"stream","text":["Model Construction...\n","INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n","...done!\n","Train...\n","...done!\n","Test...\n","INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"],"name":"stdout"},{"output_type":"error","ename":"InvalidArgumentError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)","\u001b[0;32m<ipython-input-41-1c8248d11741>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0;31m# model.load_weights('/content/drive/MyDrive/Cyber Security/Elmo/prova_felipe/Saved_weights/'+ Type + 'elmo-model' + str(fold) + '.h5')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0mbest_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'Saved/bestmodel'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfold\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.hdf5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m     \u001b[0mpredicts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbest_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m   \u001b[0;31m# print(predicts)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    906\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 908\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    909\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    910\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, model, x, batch_size, verbose, steps, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    722\u001b[0m         \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 723\u001b[0;31m         callbacks=callbacks)\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    392\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m         \u001b[0;31m# Get outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 394\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    395\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m           \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3474\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3475\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 3476\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   3477\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3478\u001b[0m     output_structure = nest.pack_sequence_as(\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1470\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1471\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1472\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1473\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1474\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mInvalidArgumentError\u001b[0m: 2 root error(s) found.\n  (0) Invalid argument: input must be a vector, got shape: []\n\t [[{{node lambda_29/module_28_apply_default/StringSplit/StringSplit}}]]\n  (1) Invalid argument: input must be a vector, got shape: []\n\t [[{{node lambda_29/module_28_apply_default/StringSplit/StringSplit}}]]\n\t [[lambda_29/module_28_apply_default/NotEqual/_2461]]\n0 successful operations.\n0 derived errors ignored."]}]}]}