{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"Elmo-K-1-3.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"I4IF9P8ChfN_","executionInfo":{"status":"ok","timestamp":1623592242645,"user_tz":-120,"elapsed":285,"user":{"displayName":"Felipe Matè","photoUrl":"","userId":"11935027497586752576"}},"outputId":"6e20a382-cb05-47fc-a9c6-c8f2344aec4f"},"source":["!nvidia-smi"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Sun Jun 13 13:50:43 2021       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 465.27       Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla P4            Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   41C    P8     7W /  75W |      0MiB /  7611MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dEVLBzZ3pzOw","executionInfo":{"status":"ok","timestamp":1623592264307,"user_tz":-120,"elapsed":17843,"user":{"displayName":"Felipe Matè","photoUrl":"","userId":"11935027497586752576"}},"outputId":"6a36e8be-5477-476a-fd8e-91dafa366e75"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"3QoIEjtk0q3B"},"source":[" # Algoritmo"]},{"cell_type":"markdown","metadata":{"id":"C-AmvDIIzS2w"},"source":["## Install"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pm0YbhvKzTBF","executionInfo":{"status":"ok","timestamp":1623592272269,"user_tz":-120,"elapsed":3792,"user":{"displayName":"Felipe Matè","photoUrl":"","userId":"11935027497586752576"}},"outputId":"ea1aded8-2261-4f8d-c240-3ab5f564520e"},"source":["pip install 'h5py<3.0.0'"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting h5py<3.0.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3f/c0/abde58b837e066bca19a3f7332d9d0493521d7dd6b48248451a9e3fe2214/h5py-2.10.0-cp37-cp37m-manylinux1_x86_64.whl (2.9MB)\n","\u001b[K     |████████████████████████████████| 2.9MB 6.9MB/s \n","\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from h5py<3.0.0) (1.15.0)\n","Requirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.7/dist-packages (from h5py<3.0.0) (1.19.5)\n","\u001b[31mERROR: tensorflow 2.5.0 has requirement h5py~=3.1.0, but you'll have h5py 2.10.0 which is incompatible.\u001b[0m\n","Installing collected packages: h5py\n","  Found existing installation: h5py 3.1.0\n","    Uninstalling h5py-3.1.0:\n","      Successfully uninstalled h5py-3.1.0\n","Successfully installed h5py-2.10.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"d8mhNt1sesQM","executionInfo":{"status":"ok","timestamp":1623592379607,"user_tz":-120,"elapsed":107340,"user":{"displayName":"Felipe Matè","photoUrl":"","userId":"11935027497586752576"}},"outputId":"b32a2d94-7033-442e-d093-a2b6708d7a54"},"source":["pip install tensorflow==1.15.0"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting tensorflow==1.15.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/92/2b/e3af15221da9ff323521565fa3324b0d7c7c5b1d7a8ca66984c8d59cb0ce/tensorflow-1.15.0-cp37-cp37m-manylinux2010_x86_64.whl (412.3MB)\n","\u001b[K     |████████████████████████████████| 412.3MB 44kB/s \n","\u001b[?25hRequirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (0.36.2)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (3.3.0)\n","Collecting keras-applications>=1.0.8\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/e3/19762fdfc62877ae9102edf6342d71b28fbfd9dea3d2f96a882ce099b03f/Keras_Applications-1.0.8-py3-none-any.whl (50kB)\n","\u001b[K     |████████████████████████████████| 51kB 8.8MB/s \n","\u001b[?25hRequirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (0.8.1)\n","Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (3.12.4)\n","Collecting tensorboard<1.16.0,>=1.15.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1e/e9/d3d747a97f7188f48aa5eda486907f3b345cd409f0a0850468ba867db246/tensorboard-1.15.0-py3-none-any.whl (3.8MB)\n","\u001b[K     |████████████████████████████████| 3.8MB 36.4MB/s \n","\u001b[?25hRequirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (1.19.5)\n","Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (1.12.1)\n","Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (0.2.0)\n","Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (1.34.1)\n","Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (1.1.2)\n","Collecting gast==0.2.2\n","  Downloading https://files.pythonhosted.org/packages/4e/35/11749bf99b2d4e3cceb4d55ca22590b0d7c2c62b9de38ac4a4a7f4687421/gast-0.2.2.tar.gz\n","Collecting tensorflow-estimator==1.15.1\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/de/62/2ee9cd74c9fa2fa450877847ba560b260f5d0fb70ee0595203082dafcc9d/tensorflow_estimator-1.15.1-py2.py3-none-any.whl (503kB)\n","\u001b[K     |████████████████████████████████| 512kB 41.2MB/s \n","\u001b[?25hRequirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (1.15.0)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (1.1.0)\n","Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (0.12.0)\n","Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras-applications>=1.0.8->tensorflow==1.15.0) (2.10.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.6.1->tensorflow==1.15.0) (57.0.0)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.0) (3.3.4)\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.0) (1.0.1)\n","Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.0) (4.0.1)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.0) (3.4.1)\n","Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.0) (3.7.4.3)\n","Building wheels for collected packages: gast\n","  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for gast: filename=gast-0.2.2-cp37-none-any.whl size=7557 sha256=4ab1440025c65a7df1f69127b87e25db1955999756b7592e8219b1d1fad0debf\n","  Stored in directory: /root/.cache/pip/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd\n","Successfully built gast\n","\u001b[31mERROR: tensorflow-probability 0.12.1 has requirement gast>=0.3.2, but you'll have gast 0.2.2 which is incompatible.\u001b[0m\n","\u001b[31mERROR: kapre 0.3.5 has requirement tensorflow>=2.0.0, but you'll have tensorflow 1.15.0 which is incompatible.\u001b[0m\n","Installing collected packages: keras-applications, tensorboard, gast, tensorflow-estimator, tensorflow\n","  Found existing installation: tensorboard 2.5.0\n","    Uninstalling tensorboard-2.5.0:\n","      Successfully uninstalled tensorboard-2.5.0\n","  Found existing installation: gast 0.4.0\n","    Uninstalling gast-0.4.0:\n","      Successfully uninstalled gast-0.4.0\n","  Found existing installation: tensorflow-estimator 2.5.0\n","    Uninstalling tensorflow-estimator-2.5.0:\n","      Successfully uninstalled tensorflow-estimator-2.5.0\n","  Found existing installation: tensorflow 2.5.0\n","    Uninstalling tensorflow-2.5.0:\n","      Successfully uninstalled tensorflow-2.5.0\n","Successfully installed gast-0.2.2 keras-applications-1.0.8 tensorboard-1.15.0 tensorflow-1.15.0 tensorflow-estimator-1.15.1\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"U6QqdON1et24","executionInfo":{"status":"ok","timestamp":1623592383834,"user_tz":-120,"elapsed":4232,"user":{"displayName":"Felipe Matè","photoUrl":"","userId":"11935027497586752576"}},"outputId":"a79c5a21-a626-4aa6-e7e6-a7f80bba9424"},"source":["pip install keras==2.5.0rc0"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting keras==2.5.0rc0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5c/1f/5ebba489d726f089d17591d56af3756a1261371f795da0e7f23dc9b4d8ac/keras-2.5.0rc0-py2.py3-none-any.whl (1.2MB)\n","\u001b[K     |████████████████████████████████| 1.2MB 5.3MB/s \n","\u001b[?25hInstalling collected packages: keras\n","  Found existing installation: Keras 2.4.3\n","    Uninstalling Keras-2.4.3:\n","      Successfully uninstalled Keras-2.4.3\n","Successfully installed keras-2.5.0rc0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TpH69jruevEv","executionInfo":{"status":"ok","timestamp":1623592387758,"user_tz":-120,"elapsed":3926,"user":{"displayName":"Felipe Matè","photoUrl":"","userId":"11935027497586752576"}},"outputId":"c65c9086-2941-4e74-b546-a6083694c7c5"},"source":["!pip install wordninja"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting wordninja\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/30/15/abe4af50f4be92b60c25e43c1c64d08453b51e46c32981d80b3aebec0260/wordninja-2.0.0.tar.gz (541kB)\n","\u001b[K     |████████████████████████████████| 542kB 5.3MB/s \n","\u001b[?25hBuilding wheels for collected packages: wordninja\n","  Building wheel for wordninja (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for wordninja: filename=wordninja-2.0.0-cp37-none-any.whl size=541553 sha256=453fc460d29c32744220e98f49b662b062c02f923e19785ec914f3bfeba450f5\n","  Stored in directory: /root/.cache/pip/wheels/22/46/06/9b6d10ed02c85e93c3bb33ac50e2d368b2586248f192a2e22a\n","Successfully built wordninja\n","Installing collected packages: wordninja\n","Successfully installed wordninja-2.0.0\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"a6qVtamazQWp"},"source":["## Imports"]},{"cell_type":"code","metadata":{"id":"qCHCGAvYzQcn"},"source":["import csv\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import pandas as pd\n","import seaborn as sn\n","import tensorflow_hub as hub\n","import tensorflow as tf\n","import tensorflow.keras\n","import wordninja as wn\n","\n","from sklearn import metrics\n","from sklearn import preprocessing\n","from sklearn.utils import shuffle\n","from sklearn.model_selection import train_test_split\n","from sklearn.model_selection import StratifiedKFold\n","\n","from tensorflow.keras.models import Model, load_model\n","from tensorflow.keras.layers import Dense,Lambda,Input\n","from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"a56hjUw_gOsA"},"source":["# Parametri"]},{"cell_type":"code","metadata":{"id":"rhT9kfmOo00G"},"source":["path = \"/content/drive/MyDrive/Cyber Security/Elmo/prova_felipe/\"\n","elmo_path = \"/content/drive/MyDrive/Cyber Security/Elmo/prova_felipe/4/\"\n","\n","batch_size = 32\n","numEpochs = 10\n","\n","start_fold = 1\n","end_fold = 11\n","\n","nfolds = 10\n","\n","nome_file = \"Dataset_Completo.csv\""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1Md_C7LcsEL9"},"source":["# Definizioni"]},{"cell_type":"code","metadata":{"id":"SieB4yCVzD7O"},"source":["def arrayToSentence(x):\n","  string=''\n","  for a in x:\n","    string=string + a + ' '\n","  return string\n","\n","def buildDataset():\n","  filecsv = open(path + nome_file, newline=\"\")\n","  lettore = csv.reader(filecsv, delimiter=\";\")\n","\n","  dataset_x = []\n","  dataset_y = []\n","  temp_y = []\n","  for a in lettore:\n","    dataset_y.append(a[0])\n","    if a[0] == 'dga':\n","      temp_y.append(1)\n","    else:\n","      temp_y.append(0)\n","    split = wn.split(a[3])\n","    sen = arrayToSentence(split)\n","    dataset_x.append(sen)\n","\n","  filecsv.close()\n","\n","  return dataset_x, dataset_y, temp_y"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Wa3XafjoSk4p"},"source":["def kfold(x, y, temp_y):\n","  # Divide the dataset into training + holdout and testing with folds\n","  sss = StratifiedKFold(n_splits=nfolds)\n","\n","  fold = 0\n","  for train, test in sss.split(x, temp_y):\n","    print(\"Writing fold \" + str(fold + 1) + \" to csv...\")\n","    fold += 1\n","    x_train, x_test, y_train, y_test, y_temp_train, y_temp_test = x[train], x[test], y[train], y[test], temp_y[train], temp_y[test]\n","    np.savetxt(path + \"Dataset_2/x_train\" + str(fold) + \".csv\", x_train, fmt='%s', delimiter=';')\n","    np.savetxt(path + \"Dataset_2/x_test\" + str(fold) + \".csv\", x_test, fmt='%s', delimiter=';')\n","    np.savetxt(path + \"Dataset_2/y_train\" + str(fold) + \".csv\", y_train, fmt='%s', delimiter=';')\n","    np.savetxt(path + \"Dataset_2/y_test\" + str(fold) + \".csv\", y_test, fmt='%s', delimiter=';')\n","    np.savetxt(path + \"Dataset_2/temp_y_train\" + str(fold) + \".csv\", y_temp_train, fmt='%i', delimiter=';')\n","    np.savetxt(path + \"Dataset_2/temp_y_test\" + str(fold) + \".csv\", y_temp_test, fmt='%i', delimiter=';')\n","  print(\"Files created\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZeMs0jxwNXjX"},"source":["def encode(le, labels):\n","    enc = le.transform(labels)\n","    return tf.keras.utils.to_categorical(enc)  #era solo keras.utils...\n","\n","def decode(le, one_hot):\n","    dec = np.argmax(one_hot, axis=1)\n","    return le.inverse_transform(dec)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xkqnMaAFgTyt"},"source":["# Preprocessing"]},{"cell_type":"code","metadata":{"id":"eLgkHmtRp-Hj"},"source":["dataset_x, dataset_y, temp_y = buildDataset()\n","dataset_x = np.array(dataset_x)\n","\n","le = preprocessing.LabelEncoder()\n","le.fit(dataset_y)\n","\n","dataset_y_encode = encode(le, dataset_y)\n","dataset_y = np.array(dataset_y_encode)\n","\n","temp_y = np.array(temp_y)\n","\n","kfold(dataset_x, dataset_y, temp_y)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pIxxcVXusLMn"},"source":["# Model"]},{"cell_type":"code","metadata":{"id":"XVZZfpNC0qlM","colab":{"base_uri":"https://localhost:8080/"},"outputId":"392d410c-528e-4687-d582-80a2abc6f275"},"source":["fold = 9\n","# sss = StratifiedKFold(n_splits=nfolds)\n","# for train, test in sss.split(dataset_x, temp_y):\n","#   fold += 1\n","#   print('Fold: ', fold)\n","#   x_train, x_test, y_train, y_test, y_train_temp, y_test_temp = dataset_x[train], dataset_x[test], dataset_y[train], dataset_y[test], temp_y[train], temp_y[test]\n","\n","# for fold in range(start_fold, end_fold):\n","print('Fold: ', fold, 'Epochs: ', numEpochs)\n","#Get fold by csv\n","x_train = np.genfromtxt(path + \"Dataset_2/x_train\" + str(fold) + \".csv\", delimiter=';', dtype=None)\n","x_test = np.genfromtxt(path + \"Dataset_2/x_test\" + str(fold) + \".csv\", delimiter=';', dtype=None)\n","y_train = np.genfromtxt(path + \"Dataset_2/y_train\" + str(fold) + \".csv\", delimiter=';', dtype=None)\n","y_test = np.genfromtxt(path + \"Dataset_2/y_test\" + str(fold) + \".csv\", delimiter=';', dtype=None)\n","y_train_temp = np.genfromtxt(path + \"Dataset_2/temp_y_train\" + str(fold) + \".csv\", delimiter=';', dtype=None)\n","y_test_temp = np.genfromtxt(path + \"Dataset_2/temp_y_test\" + str(fold) + \".csv\", delimiter=';', dtype=None)\n","\n","print('Model Construction...')\n","model = None\n","\n","#Parte costruzione del modello\n","# importo il modulo con la funzione di embedding ELMo\n","elmo = hub.Module(elmo_path)\n","\n","# Definisco la funzione di embedding\n","def ELMoEmbedding(x):\n","  return elmo(tf.squeeze(tf.cast(x, tf.string)), signature=\"default\", as_dict=True)[\"default\"]\n","\n","input_text = Input(shape=(1,), dtype=tf.string)\n","embedding = Lambda(ELMoEmbedding, output_shape=(1024, ))(input_text)\n","dense = Dense(128, activation='relu')(embedding)\n","pred = Dense(len(y_train[0]), activation='sigmoid')(dense)\n","model = Model(inputs=[input_text], outputs=pred)\n","\n","model.compile('adam', 'binary_crossentropy', metrics=['accuracy',\n","    tf.keras.metrics.AUC(),\n","    tf.keras.metrics.Precision(),\n","    tf.keras.metrics.Recall(),\n","    # tf.keras.metrics.Accuracy(), #aggiunto questo\n","    ])\n","\n","print('...done!')\n","\n","print('Train...')\n","#parte di training\n","Type = 'binary-'\n","with tf.compat.v1.Session() as session:\n","  earlystop = EarlyStopping(monitor='loss', patience=3)\n","  best_save = ModelCheckpoint(path + 'Saved/bestmodel' + str(fold) + '.hdf5', save_best_only=True, \n","                              save_weights_only=False, \n","                              monitor='val_loss', \n","                              mode='min')\n","  tf.compat.v1.keras.backend.set_session(session)\n","  session.run(tf.compat.v1.global_variables_initializer())\n","  session.run(tf.compat.v1.tables_initializer())\n","  history = model.fit(x_train,y_train,\n","            batch_size=batch_size,\n","            epochs=numEpochs, \n","            callbacks=[earlystop, best_save],\n","            validation_split=0.1\n","            )\n","  # model.save_weights('/content/drive/MyDrive/Cyber Security/Elmo/prova_felipe/Saved_weights/' + Type + 'elmo-model' + str(fold) + '.h5')\n","\n","  print('\\nhistory dict:', history.history)\n","\n","print('...done!')\n","\n","print('Test...')\n","#Parte di test\n","with tf.compat.v1.Session() as session:\n","  tf.compat.v1.keras.backend.set_session(session)\n","  session.run(tf.compat.v1.global_variables_initializer())\n","  session.run(tf.compat.v1.tables_initializer())\n","  # model.load_weights('/content/drive/MyDrive/Cyber Security/Elmo/prova_felipe/Saved_weights/'+ Type + 'elmo-model' + str(fold) + '.h5')\n","  best_model = load_model(path + 'Saved/bestmodel' + str(fold) + '.hdf5')\n","  predicts = best_model.predict(x_test, batch_size=batch_size)\n","\n","# print(predicts)\n","y_preds = decode(le, predicts)\n","y_test_temp = np.where(y_test_temp == 1, 'dga', 'legit')\n","print('...done!')\n","\n","print('Results:')\n","#Plotta i risultati\n","cm = metrics.confusion_matrix(y_test_temp, y_preds)\n","np.savetxt(path + 'Saved/confusion_matrix' + str(fold) + '.csv', cm, delimiter=',',  fmt='%i')\n","metrics1 = metrics.classification_report(y_test_temp, y_preds, output_dict=True, target_names=['legit', 'dga'])\n","print('Confusion_matrix:')\n","print(cm)\n","print('Classification_report:')\n","print(metrics1)\n","try:\n","    df1 = pd.read_csv(path + \"Saved/metrics1.csv\", index_col=[0])\n","    df1 = df1.append(pd.DataFrame(metrics1))\n","    df1.to_csv(path + \"Saved/metrics1.csv\")\n","except:\n","    pd.DataFrame(metrics1).to_csv(path + \"Saved/metrics1.csv\")\n","\n","df_cm = pd.DataFrame(cm, index = [i for i in le.classes_],\n","                  columns = [i for i in le.classes_])\n","plt.figure(1, figsize = (10,7))\n","sn.heatmap(df_cm, annot=True, fmt=\"d\")\n","plt.show()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Fold:  9 Epochs:  10\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: VisibleDeprecationWarning: Reading unicode strings without specifying the encoding argument is deprecated. Set the encoding, use None for the system default.\n","  # This is added back by InteractiveShellApp.init_path()\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:12: VisibleDeprecationWarning: Reading unicode strings without specifying the encoding argument is deprecated. Set the encoding, use None for the system default.\n","  if sys.path[0] == '':\n"],"name":"stderr"},{"output_type":"stream","text":["Model Construction...\n","INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n","WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n","Instructions for updating:\n","If using Keras pass *_constraint arguments to layers.\n","WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.where in 2.0, which has the same broadcast rule as np.where\n","...done!\n","Train...\n","Train on 546668 samples, validate on 60741 samples\n","Epoch 1/10\n","546668/546668 [==============================] - 1800s 3ms/sample - loss: 0.1814 - acc: 0.9279 - auc: 0.9797 - precision: 0.9279 - recall: 0.9279 - val_loss: 0.1434 - val_acc: 0.9420 - val_auc: 0.9872 - val_precision: 0.9418 - val_recall: 0.9422\n","Epoch 2/10\n","546668/546668 [==============================] - 1782s 3ms/sample - loss: 0.1386 - acc: 0.9465 - auc: 0.9878 - precision: 0.9465 - recall: 0.9465 - val_loss: 0.1416 - val_acc: 0.9444 - val_auc: 0.9876 - val_precision: 0.9443 - val_recall: 0.9446\n","Epoch 3/10\n","546668/546668 [==============================] - 1789s 3ms/sample - loss: 0.1232 - acc: 0.9528 - auc: 0.9902 - precision: 0.9528 - recall: 0.9528 - val_loss: 0.1674 - val_acc: 0.9352 - val_auc: 0.9837 - val_precision: 0.9352 - val_recall: 0.9352\n","Epoch 4/10\n","546668/546668 [==============================] - 1770s 3ms/sample - loss: 0.1125 - acc: 0.9573 - auc: 0.9917 - precision: 0.9573 - recall: 0.9574 - val_loss: 0.1360 - val_acc: 0.9468 - val_auc: 0.9884 - val_precision: 0.9468 - val_recall: 0.9468\n","Epoch 5/10\n","546668/546668 [==============================] - 1773s 3ms/sample - loss: 0.1047 - acc: 0.9603 - auc: 0.9928 - precision: 0.9603 - recall: 0.9603 - val_loss: 0.2969 - val_acc: 0.8927 - val_auc: 0.9585 - val_precision: 0.8926 - val_recall: 0.8927\n","Epoch 6/10\n","546668/546668 [==============================] - 1775s 3ms/sample - loss: 0.0985 - acc: 0.9626 - auc: 0.9936 - precision: 0.9626 - recall: 0.9627 - val_loss: 0.1414 - val_acc: 0.9462 - val_auc: 0.9879 - val_precision: 0.9462 - val_recall: 0.9462\n","Epoch 7/10\n","546668/546668 [==============================] - 1776s 3ms/sample - loss: 0.0927 - acc: 0.9652 - auc: 0.9942 - precision: 0.9652 - recall: 0.9652 - val_loss: 0.1153 - val_acc: 0.9595 - val_auc: 0.9912 - val_precision: 0.9595 - val_recall: 0.9595\n","Epoch 8/10\n","546668/546668 [==============================] - 1814s 3ms/sample - loss: 0.0877 - acc: 0.9669 - auc: 0.9948 - precision: 0.9669 - recall: 0.9669 - val_loss: 0.1311 - val_acc: 0.9559 - val_auc: 0.9890 - val_precision: 0.9558 - val_recall: 0.9559\n","Epoch 9/10\n"," 80448/546668 [===>..........................] - ETA: 23:02 - loss: 0.0811 - acc: 0.9691 - auc: 0.9955 - precision: 0.9691 - recall: 0.9691"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ipiAAAhBo5Gj"},"source":[" # Test"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":702},"id":"R1D86xE-fCmM","executionInfo":{"status":"error","timestamp":1623510353094,"user_tz":-120,"elapsed":291094,"user":{"displayName":"Felipe Matè","photoUrl":"","userId":"07797625908934989507"}},"outputId":"509d0cd9-3a8e-4721-ead6-801e3cdff424"},"source":["fold = 0\n","sss = StratifiedKFold(n_splits=nfolds)\n","for train, test in sss.split(dataset_x, temp_y):\n","  fold += 1\n","  print('Fold: ', fold)\n","  x_train, x_test, y_train, y_test, y_train_temp, y_test_temp = dataset_x[train], dataset_x[test], dataset_y[train], dataset_y[test], temp_y[train], temp_y[test]\n","\n","# for fold in range(start_fold, end_fold):\n","#   print('Fold: ', fold, 'Epochs: ', numEpochs)\n","\n","#   x_train = np.genfromtxt(path + \"Dataset/x_train\" + str(fold) + \".csv\", delimiter=';', dtype=None)\n","#   x_test = np.genfromtxt(path + \"Dataset/x_test\" + str(fold) + \".csv\", delimiter=';', dtype=None)\n","#   y_train = np.genfromtxt(path + \"Dataset/y_train\" + str(fold) + \".csv\", delimiter=';', dtype=None)\n","#   y_test = np.genfromtxt(path + \"Dataset/y_test\" + str(fold) + \".csv\", delimiter=';', dtype=None)\n","#   y_train_temp = np.genfromtxt(path + \"Dataset/temp_y_train\" + str(fold) + \".csv\", delimiter=';', dtype=None)\n","#   y_test_temp = np.genfromtxt(path + \"Dataset/temp_y_test\" + str(fold) + \".csv\", delimiter=';', dtype=None)\n","\n","  #Parte costruzione del modello\n","  # importo il modulo con la funzione di embedding ELMo\n","\n","  if fold != 1 and fold < 9:\n","    continue\n","\n","  elmo = hub.Module(elmo_path)\n","\n","  # Definisco la funzione di embedding\n","  def ELMoEmbedding(x):\n","    return elmo(tf.squeeze(tf.cast(x, tf.string)), signature=\"default\", as_dict=True)[\"default\"]\n","\n","  # input_text = Input(shape=(1,), dtype=tf.string)\n","  # embedding = Lambda(ELMoEmbedding, output_shape=(1024, ))(input_text)\n","  # dense = Dense(128, activation='relu')(embedding)\n","  # pred = Dense(len(y_train[0]), activation='sigmoid')(dense)\n","  # model = Model(inputs=[input_text], outputs=pred)\n","\n","  # model.compile('adam', 'binary_crossentropy', metrics=['accuracy',\n","  #     tf.keras.metrics.AUC(),\n","  #     tf.keras.metrics.Precision(),\n","  #     tf.keras.metrics.Recall(),\n","  #     # tf.keras.metrics.Accuracy(), #aggiunto questo\n","  #     ])\n","\n","  # print('...done!')\n","\n","  # print('Train...')\n","  # #parte di training\n","  # Type = 'binary-'\n","  # with tf.compat.v1.Session() as session:\n","  #   earlystop = EarlyStopping(monitor='loss', patience=3)\n","  #   best_save = ModelCheckpoint(path + 'Saved/bestmodel' + str(fold) + '.hdf5', save_best_only=True, \n","  #                               save_weights_only=False, \n","  #                               monitor='val_loss', \n","  #                               mode='min')\n","  #   tf.compat.v1.keras.backend.set_session(session)\n","  #   session.run(tf.compat.v1.global_variables_initializer())\n","  #   session.run(tf.compat.v1.tables_initializer())\n","  #   history = model.fit(x_train,y_train,\n","  #             batch_size=batch_size,\n","  #             epochs=numEpochs, \n","  #             callbacks=[earlystop, best_save],\n","  #             validation_split=0.1\n","  #             )\n","  #   # model.save_weights('/content/drive/MyDrive/Cyber Security/Elmo/prova_felipe/Saved_weights/' + Type + 'elmo-model' + str(fold) + '.h5')\n","\n","  #   print('\\nhistory dict:', history.history)\n","\n","  # print('...done!')\n","\n","  print(x_test)\n","\n","  print('Test...')\n","  #Parte di test\n","  with tf.compat.v1.Session() as session:\n","    tf.compat.v1.keras.backend.set_session(session)\n","    session.run(tf.compat.v1.global_variables_initializer())\n","    session.run(tf.compat.v1.tables_initializer())\n","    # model.load_weights('/content/drive/MyDrive/Cyber Security/Elmo/prova_felipe/Saved_weights/'+ Type + 'elmo-model' + str(fold) + '.h5')\n","    best_model = load_model(path + 'Saved/bestmodel' + str(fold) + '.hdf5')\n","    predicts = best_model.predict(x_test, batch_size=batch_size)\n","\n","  # print(predicts)\n","  y_preds = decode(le, predicts)\n","  y_test_temp = np.where(y_test_temp == 1, 'dga', 'legit')\n","  print('...done!')\n","\n","  print('Results:')\n","  #Plotta i risultati\n","  # cm = metrics.confusion_matrix(y_test_temp, y_preds)\n","  # np.savetxt(path + 'Saved/confusion_matrix' + str(fold) + '.csv', cm, delimiter=',',  fmt='%i')\n","  # metrics1 = metrics.classification_report(y_test_temp, y_preds, output_dict=True, target_names=['legit', 'dga'])\n","  # print('Confusion_matrix:')\n","  # print(cm)\n","  # print('Classification_report:')\n","  # print(metrics1)\n","  # try:\n","  #     df1 = pd.read_csv(path + \"Saved/metrics1.csv\", index_col=[0])\n","  #     df1 = df1.append(pd.DataFrame(metrics1))\n","  #     df1.to_csv(path + \"Saved/metrics1.csv\")\n","  # except:\n","  #     pd.DataFrame(metrics1).to_csv(path + \"Saved/metrics1.csv\")\n","\n","  # df_cm = pd.DataFrame(cm, index = [i for i in le.classes_],\n","  #                   columns = [i for i in le.classes_])\n","  # plt.figure(1, figsize = (10,7))\n","  # sn.heatmap(df_cm, annot=True, fmt=\"d\")\n","  # plt.show()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Fold:  1\n","['plastic bags s a com ' 'm zl track com ' 'miss slim ru ' ...\n"," 'hh h hr x yy m ooo com ' 'mm jia j moj m uz mo com '\n"," 'dv qom u aq yn com ']\n","Test...\n","INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n","...done!\n","Results:\n","Fold:  2\n","Fold:  3\n","Fold:  4\n","Fold:  5\n","Fold:  6\n","Fold:  7\n","Fold:  8\n","Fold:  9\n","['the peak magazine com sg ' 'furry tips com ' 'reuse co 2 com ' ...\n"," 'go x as eh info ' 'od so ten woc a com ' 'zi ey qg haj my ']\n","Test...\n","INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"],"name":"stdout"},{"output_type":"error","ename":"InvalidArgumentError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)","\u001b[0;32m<ipython-input-31-d20e28617820>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0;31m# model.load_weights('/content/drive/MyDrive/Cyber Security/Elmo/prova_felipe/Saved_weights/'+ Type + 'elmo-model' + str(fold) + '.h5')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0mbest_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'Saved/bestmodel'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfold\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.hdf5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m     \u001b[0mpredicts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbest_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m   \u001b[0;31m# print(predicts)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    906\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 908\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    909\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    910\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, model, x, batch_size, verbose, steps, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    722\u001b[0m         \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 723\u001b[0;31m         callbacks=callbacks)\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    392\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m         \u001b[0;31m# Get outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 394\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    395\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m           \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3474\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3475\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 3476\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   3477\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3478\u001b[0m     output_structure = nest.pack_sequence_as(\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1470\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1471\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1472\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1473\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1474\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mInvalidArgumentError\u001b[0m: input must be a vector, got shape: []\n\t [[{{node lambda_8/module_9_apply_default/StringSplit/StringSplit}}]]"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":637},"id":"SHGKq5Igw3k5","executionInfo":{"status":"error","timestamp":1623513285846,"user_tz":-120,"elapsed":180724,"user":{"displayName":"Felipe Matè","photoUrl":"","userId":"07797625908934989507"}},"outputId":"c62d28ce-8f1f-4258-fe7d-e784b63ebe12"},"source":["# fold = 0\n","# sss = StratifiedKFold(n_splits=nfolds)\n","# for train, test in sss.split(dataset_x, temp_y):\n","#   fold += 1\n","#   print('Fold: ', fold)\n","#   x_train, x_test, y_train, y_test, y_train_temp, y_test_temp = dataset_x[train], dataset_x[test], dataset_y[train], dataset_y[test], temp_y[train], temp_y[test]\n","\n","start_fold, end_fold = 9, 10\n","\n","for fold in range(start_fold, end_fold):\n","  print('Fold: ', fold, 'Epochs: ', numEpochs)\n","  #Get fold by csv\n","  x_train = np.genfromtxt(path + \"Dataset/x_train\" + str(fold) + \".csv\", delimiter=';', dtype=None)\n","  x_test = np.genfromtxt(path + \"Dataset/x_test\" + str(fold) + \".csv\", delimiter=';', dtype=None)\n","  y_train = np.genfromtxt(path + \"Dataset/y_train\" + str(fold) + \".csv\", delimiter=';', dtype=None)\n","  y_test = np.genfromtxt(path + \"Dataset/y_test\" + str(fold) + \".csv\", delimiter=';', dtype=None)\n","  y_train_temp = np.genfromtxt(path + \"Dataset/temp_y_train\" + str(fold) + \".csv\", delimiter=';', dtype=None)\n","  y_test_temp = np.genfromtxt(path + \"Dataset/temp_y_test\" + str(fold) + \".csv\", delimiter=';', dtype=None)\n","\n","  if fold < 9:\n","    continue\n","\n","  print('Model Construction...')\n","  model = None\n","\n","  #Parte costruzione del modello\n","  # importo il modulo con la funzione di embedding ELMo\n","  elmo = hub.Module(elmo_path)\n","\n","  # Definisco la funzione di embedding\n","  def ELMoEmbedding(x):\n","    return elmo(tf.squeeze(tf.cast(x, tf.string)), signature=\"default\", as_dict=True)[\"default\"]\n","\n","  input_text = Input(shape=(1,), dtype=tf.string)\n","  embedding = Lambda(ELMoEmbedding, output_shape=(1024, ))(input_text)\n","  dense = Dense(128, activation='relu')(embedding)\n","  pred = Dense(len(y_train[0]), activation='sigmoid')(dense)\n","  model = Model(inputs=[input_text], outputs=pred)\n","\n","  model.compile('adam', 'binary_crossentropy', metrics=['accuracy',\n","      tf.keras.metrics.AUC(),\n","      tf.keras.metrics.Precision(),\n","      tf.keras.metrics.Recall(),\n","      # tf.keras.metrics.Accuracy(), #aggiunto questo\n","      ])\n","\n","  print('...done!')\n","\n","  # print('Train...')\n","  # #parte di training\n","  # Type = 'binary-'\n","  # with tf.compat.v1.Session() as session:\n","  #   earlystop = EarlyStopping(monitor='loss', patience=3)\n","  #   best_save = ModelCheckpoint(path + 'Saved/bestmodel' + str(fold) + '.hdf5', save_best_only=True, \n","  #                               save_weights_only=False, \n","  #                               monitor='val_loss', \n","  #                               mode='min')\n","  #   tf.compat.v1.keras.backend.set_session(session)\n","  #   session.run(tf.compat.v1.global_variables_initializer())\n","  #   session.run(tf.compat.v1.tables_initializer())\n","  #   history = model.fit(x_train,y_train,\n","  #             batch_size=batch_size,\n","  #             epochs=numEpochs, \n","  #             callbacks=[earlystop, best_save],\n","  #             validation_split=0.1\n","  #             )\n","  #   # model.save_weights('/content/drive/MyDrive/Cyber Security/Elmo/prova_felipe/Saved_weights/' + Type + 'elmo-model' + str(fold) + '.h5')\n","\n","  #   print('\\nhistory dict:', history.history)\n","\n","  # print('...done!')\n","\n","  print('Test...', path + 'Saved/bestmodel_' + str(fold) + '_1.hdf5')\n","  #Parte di test\n","  with tf.compat.v1.Session() as session:\n","    tf.compat.v1.keras.backend.set_session(session)\n","    session.run(tf.compat.v1.global_variables_initializer())\n","    session.run(tf.compat.v1.tables_initializer())\n","    # model.load_weights('/content/drive/MyDrive/Cyber Security/Elmo/prova_felipe/Saved_weights/'+ Type + 'elmo-model' + str(fold) + '.h5')\n","    best_model = load_model(path + 'Saved/bestmodel_' + str(fold) + '_1.hdf5')\n","    predicts = best_model.predict(x_test, batch_size=batch_size)\n","\n","  # print(predicts)\n","  y_preds = decode(le, predicts)\n","  y_test_temp = np.where(y_test_temp == 1, 'dga', 'legit')\n","  print('...done!')\n","\n","  # print('Results:')\n","  # #Plotta i risultati\n","  # cm = metrics.confusion_matrix(y_test_temp, y_preds)\n","  # np.savetxt(path + 'Saved/confusion_matrix' + str(fold) + '.csv', cm, delimiter=',',  fmt='%i')\n","  # metrics1 = metrics.classification_report(y_test_temp, y_preds, output_dict=True, target_names=['legit', 'dga'])\n","  # print('Confusion_matrix:')\n","  # print(cm)\n","  # print('Classification_report:')\n","  # print(metrics1)\n","  # try:\n","  #     df1 = pd.read_csv(path + \"Saved/metrics1.csv\", index_col=[0])\n","  #     df1 = df1.append(pd.DataFrame(metrics1))\n","  #     df1.to_csv(path + \"Saved/metrics1.csv\")\n","  # except:\n","  #     pd.DataFrame(metrics1).to_csv(path + \"Saved/metrics1.csv\")\n","\n","  # df_cm = pd.DataFrame(cm, index = [i for i in le.classes_],\n","  #                   columns = [i for i in le.classes_])\n","  # plt.figure(1, figsize = (10,7))\n","  # sn.heatmap(df_cm, annot=True, fmt=\"d\")\n","  # plt.show()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Fold:  9 Epochs:  10\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:13: VisibleDeprecationWarning: Reading unicode strings without specifying the encoding argument is deprecated. Set the encoding, use None for the system default.\n","  del sys.path[0]\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:14: VisibleDeprecationWarning: Reading unicode strings without specifying the encoding argument is deprecated. Set the encoding, use None for the system default.\n","  \n"],"name":"stderr"},{"output_type":"stream","text":["Model Construction...\n","INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n","...done!\n","Test... /content/drive/MyDrive/Cyber Security/Elmo/prova_felipe/Saved/bestmodel_9_1.hdf5\n","INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"],"name":"stdout"},{"output_type":"error","ename":"InvalidArgumentError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)","\u001b[0;32m<ipython-input-36-6bea5aaa324e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0;31m# model.load_weights('/content/drive/MyDrive/Cyber Security/Elmo/prova_felipe/Saved_weights/'+ Type + 'elmo-model' + str(fold) + '.h5')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[0mbest_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'Saved/bestmodel_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfold\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'_1.hdf5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m     \u001b[0mpredicts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbest_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m   \u001b[0;31m# print(predicts)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    906\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 908\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    909\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    910\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, model, x, batch_size, verbose, steps, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    722\u001b[0m         \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 723\u001b[0;31m         callbacks=callbacks)\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    392\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m         \u001b[0;31m# Get outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 394\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    395\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m           \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3474\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3475\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 3476\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   3477\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3478\u001b[0m     output_structure = nest.pack_sequence_as(\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1470\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1471\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1472\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1473\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1474\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mInvalidArgumentError\u001b[0m: 2 root error(s) found.\n  (0) Invalid argument: input must be a vector, got shape: []\n\t [[{{node lambda_14/module_15_apply_default/StringSplit/StringSplit}}]]\n  (1) Invalid argument: input must be a vector, got shape: []\n\t [[{{node lambda_14/module_15_apply_default/StringSplit/StringSplit}}]]\n\t [[lambda_14/module_15_apply_default/map/TensorArrayStack/TensorArrayGatherV3/_1455]]\n0 successful operations.\n0 derived errors ignored."]}]},{"cell_type":"markdown","metadata":{"id":"4jLtyZ76Zqir"},"source":["# SAVIO"]},{"cell_type":"code","metadata":{"id":"cxXtUHfDZ1tS"},"source":["import csv\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import pandas as pd\n","import seaborn as sn\n","import tensorflow_hub as hub\n","import tensorflow as tf\n","import tensorflow.keras\n","import wordninja as wn\n","\n","from sklearn import metrics\n","from sklearn import preprocessing\n","from sklearn.utils import shuffle\n","from sklearn.model_selection import train_test_split\n","from sklearn.model_selection import StratifiedKFold\n","\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.layers import Dense,Lambda,Input"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hhbMjq2PZ2qJ"},"source":["nome_file_savio = \"Dataset_Piccolo.csv\"\n","def buildBinary():\n","  filecsv = open('/content/drive/MyDrive/Cyber Security/Elmo/prova_felipe/' + nome_file_savio, newline=\"\")\n","  lettore = csv.reader(filecsv, delimiter=\";\")\n","\n","  sentencesTemp=[]\n","  labelsTemp=[]\n","\n","  x_train=[]\n","  y_train=[]\n","  x_test=[]\n","  y_test=[]\n","\n","  labelTemp=''\n","  count = 0\n","\n","  dataset_x = []\n","  dataset_y = []\n","  for a in lettore:\n","    dataset_y.append(a[0])\n","    split = wn.split(a[3])\n","    sen = arrayToSentence(split)\n","    dataset_x.append(sen)\n","    if labelTemp=='':\n","      labelTemp=a[0]\n","      splitted = wn.split(a[3])\n","      sentence = arrayToSentence(splitted)\n","      sentencesTemp.append(sentence)\n","      labelsTemp.append(a[0])\n","    elif labelTemp==a[0]:\n","      splitted = wn.split(a[3])\n","      sentence = arrayToSentence(splitted)\n","      sentencesTemp.append(sentence)\n","      labelsTemp.append(a[0])\n","    elif a[0] != labelTemp:\n","      # print(sentencesTemp)\n","      # print(len(sentencesTemp))\n","      x_trainTemp, x_testTemp, y_trainTemp, y_testTemp = train_test_split(sentencesTemp,labelsTemp,test_size=0.6)\n","      x_train=x_train + x_trainTemp\n","      x_test=x_test + x_testTemp\n","      y_train=y_train + y_trainTemp\n","      y_test=y_test + y_testTemp\n","      sentencesTemp=[]\n","      labelsTemp=[]\n","      labelTemp=a[0]\n","      splitted = wn.split(a[3])\n","      sentence = arrayToSentence(splitted)\n","      sentencesTemp.append(sentence)\n","      labelsTemp.append(a[0])\n","      if count == 0:\n","        count = count + 1;\n","        # print(len(x_trainTemp))\n","        # print(x_train)\n","        # print(x_test)\n","        # print(y_train)\n","        # print(y_test)\n","\n","  filecsv.close()\n","  # print(count)\n","  # print(dataset_x)\n","  # print(len(dataset_x))\n","  # print(dataset_y)\n","  # print(len(dataset_y))\n","\n","  x_trainTemp, x_testTemp, y_trainTemp, y_testTemp = train_test_split(sentencesTemp,labelsTemp,test_size=0.6)\n","  x_train=x_train + x_trainTemp\n","  x_test=x_test + x_testTemp\n","  y_train=y_train + y_trainTemp\n","  y_test=y_test + y_testTemp\n","\n","  x_train, y_train = shuffle(x_train, y_train)\n","  x_test, y_test = shuffle(x_test, y_test)\n","\n","\n","  return x_train, x_test, y_train, y_test"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"utmJE5jYZsWM"},"source":["batch_size = 32\n","numEpochs = 1 #10\n","\n","print('Dataset Building...')\n","\n","#x_train, x_test, y, y_test=DsB.build()\n","#Type = 'paper-matsnu-binary-'\n","\n","x_train, x_test, y, y_test=buildBinary()\n","Type = 'binary-'\n","\n","#x_train, x_test, y, y_test=DsBM.buildMulti()\n","#Type = 'multiclass-'\n","\n","\n","x_train=np.array(x_train)\n","x_test=np.array(x_test)\n","\n","\n","le = preprocessing.LabelEncoder()\n","le.fit(y)\n","\n","# Definisco le funzioni per codificare e decodificare le labels\n","def encode(le, labels):\n","    enc = le.transform(labels)\n","    return tf.keras.utils.to_categorical(enc)  #era solo keras.utils...\n","\n","def decode(le, one_hot):\n","    dec = np.argmax(one_hot, axis=1)\n","    return le.inverse_transform(dec)\n","\n","y_enc = encode(le, y)\n","\n","y_train=np.array(y_enc)\n","\n","# print(y_enc)\n","# print(len(y_enc))\n","# print(y_train)\n","# print(len(y_train))\n","\n","print(x_train)\n","print(y_train)\n","\n","print('...done!')\n","\n","print(len(y_enc[0]))\n","\n","print('Model Construction...')\n","#Parte costruzione del modello\n","# importo il modulo con la funzione di embedding ELMo\n","elmo = hub.Module(\"/content/drive/MyDrive/Cyber Security/Elmo/prova_felipe/4\")\n","\n","# Definisco la funzione di embedding\n","def ELMoEmbedding(x):\n","    return elmo(tf.squeeze(tf.cast(x, tf.string)), signature=\"default\", as_dict=True)[\"default\"]\n","\n","\n","input_text = Input(shape=(1,), dtype=tf.string)\n","embedding = Lambda(ELMoEmbedding, output_shape=(1024, ))(input_text)\n","dense=Dense(128, activation='relu')(embedding)\n","pred=Dense(len(y_enc[0]), activation='sigmoid')(dense)\n","model=Model(inputs=[input_text], outputs=pred)\n","\n","\n","model.compile('adam', 'binary_crossentropy', metrics=['accuracy',\n","    tf.keras.metrics.AUC(),\n","    tf.keras.metrics.Precision(),\n","    tf.keras.metrics.Recall(),\n","    tf.keras.metrics.Accuracy(), #aggiunto questo\n","    ])\n","\n","print('...done!')\n","\n","print('Train...')\n","#parte di training\n","with tf.compat.v1.Session() as session:\n","    tf.compat.v1.keras.backend.set_session(session)\n","    session.run(tf.compat.v1.global_variables_initializer())\n","    session.run(tf.compat.v1.tables_initializer())\n","    history = model.fit(x_train,y_train,\n","              batch_size=batch_size,\n","              epochs=numEpochs,\n","                #validation_data=[x_val, y_val]\n","              )\n","    model.save_weights('/content/drive/MyDrive/Cyber Security/Elmo/prova_felipe/Saved_weights/' + Type + 'elmo-model.h5')\n","\n","    print('\\nhistory dict:', history.history)\n","\n","print('...done!')\n","\n","print('Test...')\n","print(x_test)\n","print(len(x_test))\n","#Parte di testing\n","with tf.compat.v1.Session() as session:\n","    tf.compat.v1.keras.backend.set_session(session)\n","    session.run(tf.compat.v1.global_variables_initializer())\n","    session.run(tf.compat.v1.tables_initializer())\n","    model.load_weights('/content/drive/MyDrive/Cyber Security/Elmo/prova_felipe/Saved_weights/'+ Type + 'elmo-model.h5')\n","    predicts = model.predict(x_test, batch_size=batch_size)\n","\n","print(predicts)\n","y_preds = decode(le, predicts)\n","\n","print(y_preds)\n","print(y_test)\n","\n","print(len(predicts))\n","print(len(y_preds))\n","\n","print('...done!')\n","\n","print('Results:')\n","#Plotta i risultati\n","cm = metrics.confusion_matrix(y_test, y_preds)\n","print(metrics.classification_report(y_test, y_preds))\n","\n","df_cm = pd.DataFrame(cm, index = [i for i in le.classes_],\n","                  columns = [i for i in le.classes_])\n","plt.figure(1, figsize = (10,7))\n","sn.heatmap(df_cm, annot=True, fmt=\"d\")\n","plt.show()\n","\n","#fpr_keras, tpr_keras, thresholds_keras = metrics.roc_curve(y_test.ravel(), predicts.ravel())\n","#auc_keras = auc(fpr_keras, tpr_keras)\n","\n","#plotta i grafici\n","#plt.figure(2, figsize = (10,7))\n","#plt.plot([0, 1], [0, 1], 'k--')\n","#plt.plot(fpr_keras, tpr_keras, label='Binary (area = {:.3f})'.format(auc_keras))\n","#plt.xlabel('False positive rate')\n","#plt.ylabel('True positive rate')\n","#plt.title('ROC curve')\n","#plt.legend(loc='best')\n","#plt.show()"],"execution_count":null,"outputs":[]}]}